{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Lily Grumbach\n",
    "    <br>M1 Humanités numériques - Université PSL</h4>\n",
    "<h1><center>Rendu TAIS-TAL </center></h1>\n",
    "<h2><center>Partie TAL</center></h2>\n",
    "<h3><center>2/3 : Reconnaissance d'entités nommées sur Doccano</center></h3>\n",
    "\n",
    "<b><u>PLAN:</u></b>\n",
    "1) Exploration des données\n",
    "<br>\n",
    "**2) NER avec Doccano**\n",
    "\n",
    "    a) initialisation des données\n",
    "    b) Exploration de la NER préalable de Spacy\n",
    "    c) Mise en place de la stratégie pour l'annotation sur Doccano\n",
    "    d) Entraînement par 5-fold cross validation\n",
    "\n",
    "3) Application du NER au df\n",
    "\n",
    "**Enjeux de la partie 2:**\n",
    "Les deux revues scientifiques étudiées reposent sur trois éléments consitutifs: \n",
    "- **Thématiques** : \n",
    "    - Hygiène \n",
    "    - Pathologies dites \"exotiques\" ou \"coloniales\"\n",
    "    - Liens avec les sciences humaines et sociales \n",
    "    - Relations avec les découvertes récentes\n",
    "- **Terrains spécifiques**  : \n",
    "    - Colonies et protectorats français\n",
    "    - Batiments navals de la marine française\n",
    "    - Autres territoires hors métropole et possessions françaises\n",
    "    - Dans certains cas des laboratoires\n",
    "- **Corps de métier** Population particulière de rédacteurs que sont les membres du corps de santé des colonies et du corps de médecine navale(fait l'objet d'une analyse de réseau non prise en considération dans ce devoir)\n",
    "\n",
    "\n",
    "**Objectifs de la partie 2:**\n",
    "- Entraîner une NER sur notre BDD\n",
    "\n",
    "<br></br>\n",
    "**L'objectif à long terme** de cet entraînement NER est de :\n",
    "- Mettre en place une NER qui pourrait être appliquée au full text de chacune des revues une fois les OCR nettoyées. \n",
    "- Appliquer cette NER à la revue des Archives de médecine navale et coloniale (1892-1898)\n",
    "- Alimenter une future base de données relationnelle\n",
    "\n",
    "\n",
    "**Limites de mon travail:** Il me sera nécessaire de mesurer si elle fonctionne bien à la fois sur le reste de la période étudiée ainsi que sur le Bulletin de la Société de Pathologie exotique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Généraux : \n",
    "import spacy\n",
    "import fr_core_news_md\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "#Importation de mon module \n",
    "import Training_Doccano\n",
    "from Training_Doccano import pandas_to_doccano\n",
    "\n",
    "#Autres\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des dataframes \n",
    "dfAHMC = pd.read_csv(\"./Notebook_data/input/AHMC_articles-originaux_Termine2.tsv\" ,sep='\\t')\n",
    "\n",
    "df_spacyAHMC = dfAHMC.loc[dfAHMC[\"revue_annee\"]<= 1908]\n",
    "df_spacyAHMC = df_spacyAHMC[[\"article_titre\",\"revue_annee\"]]\n",
    "\n",
    "dfAMN=pd.read_csv(\"./Notebook_data/input/AMN-articles-98-08_clean-main.tsv\",sep=\"\\t\")\n",
    "dfAMN=dfAMN[:-2] #les deux derniers rows étaient des nan\n",
    "df_spacyAMN = dfAMN[[\"article_titre\",\"revue_annee\"]]\n",
    "\n",
    "\n",
    "#print(\"Nombre d'articles dans chaque revue de 1898 à 1908:\",\n",
    "#       \"\\nAHMC :\",len(df_spacyAHMC),\n",
    "#       \"\\nAMN : \",len(df_spacyAMN),\n",
    "#       \"\\nTotal à traiter : \",len(df_spacyAHMC)+len(df_spacyAMN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On cheche dans un premier temps à avoir une vision générale des annotations du texte et non spécifiquement par article\n",
    "#Import du full text de chaque revue. \n",
    "with open(\"./Notebook_data/titres_articles_full_AHMC-to-1908.txt\") as f:  \n",
    "    AHMC = f.readlines()\n",
    "    AHMC_txt = AHMC[0]\n",
    "\n",
    "with open(\"./Notebook_data/titres_articles_full_AMN-1898-1908.txt\") as f:\n",
    "    AMN = f.readlines()\n",
    "    AMN_txt = AMN[0]\n",
    "    \n",
    "Comparaison_txt = AMN_txt + AHMC_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explorations avec le pretrained model de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importation du pretrained model en français\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "docAHMC = nlp(AHMC_txt)\n",
    "docAMN = nlp(AMN_txt)\n",
    "docComparaison = nlp(Comparaison_txt)\n",
    "type(docAHMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ents(doc): \n",
    "    L=[]\n",
    "    if doc.ents: \n",
    "        for ent in doc.ents: \n",
    "            annotations=ent.text+' - ' +str(ent.start_char) +' - '+ str(ent.end_char) +' - '+ent.label_+ ' - '+str(spacy.explain(ent.label_))\n",
    "            L.append(annotations)\n",
    "#             print(annotations)\n",
    "#             print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_ents(docComparaison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "# displacy.render(docComparaison, style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Training_Doccano import list_ents_by_label\n",
    "LOCentities = list_ents_by_label(docComparaison,\"LOC\")\n",
    "ORGentities = list_ents_by_label(docComparaison,\"ORG\")\n",
    "PERentities = list_ents_by_label(docComparaison,\"PER\")\n",
    "MISCentities = list_ents_by_label(docComparaison,\"MISC\")\n",
    "\n",
    "#On remarque une grande disparité de tailles \n",
    "# print(\"Taille de chaque label pour l'ensemble des textes:\",\n",
    "#       \"\\n LOC:\",len(LOCentities),\n",
    "#       \"\\n PER :\",len(PERentities),\n",
    "#       \"\\n MISC :\",len(MISCentities),\n",
    "#       \"\\n ORG :\",len(ORGentities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "      <th>MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dr L. Plazy</td>\n",
       "      <td>Service médical</td>\n",
       "      <td>docteur Quoy</td>\n",
       "      <td>De l'origine toxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr Auffret</td>\n",
       "      <td>Septième conférence internationale de la Croix...</td>\n",
       "      <td>M. E. Maurel</td>\n",
       "      <td>Dr Titi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buenos</td>\n",
       "      <td>Dr Valence</td>\n",
       "      <td>Le Hunte</td>\n",
       "      <td>Dr Tatsusaburo Yabé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Surra</td>\n",
       "      <td>Dr BelletChirurgie</td>\n",
       "      <td>Dr Richer de Forges</td>\n",
       "      <td>La lutte contre la peste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subsistances</td>\n",
       "      <td>Service de la vaccine</td>\n",
       "      <td>Dr Liffran</td>\n",
       "      <td>Exposition d'hygiène</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>4</td>\n",
       "      <td>421</td>\n",
       "      <td>100</td>\n",
       "      <td>Dr TitiNote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>5</td>\n",
       "      <td>422</td>\n",
       "      <td>101</td>\n",
       "      <td>Délivrance de ceintures de flanelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>6</td>\n",
       "      <td>423</td>\n",
       "      <td>102</td>\n",
       "      <td>Dr Suard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>7</td>\n",
       "      <td>424</td>\n",
       "      <td>103</td>\n",
       "      <td>NolletÉtude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>8</td>\n",
       "      <td>425</td>\n",
       "      <td>104</td>\n",
       "      <td>Dr SoulsNote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              LOC                                                ORG  \\\n",
       "0     Dr L. Plazy                                    Service médical   \n",
       "1      Dr Auffret  Septième conférence internationale de la Croix...   \n",
       "2          Buenos                                         Dr Valence   \n",
       "3           Surra                                 Dr BelletChirurgie   \n",
       "4    Subsistances                              Service de la vaccine   \n",
       "..            ...                                                ...   \n",
       "514             4                                                421   \n",
       "515             5                                                422   \n",
       "516             6                                                423   \n",
       "517             7                                                424   \n",
       "518             8                                                425   \n",
       "\n",
       "                     PER                                 MISC  \n",
       "0           docteur Quoy                    De l'origine toxi  \n",
       "1           M. E. Maurel                              Dr Titi  \n",
       "2               Le Hunte                  Dr Tatsusaburo Yabé  \n",
       "3    Dr Richer de Forges             La lutte contre la peste  \n",
       "4             Dr Liffran                 Exposition d'hygiène  \n",
       "..                   ...                                  ...  \n",
       "514                  100                          Dr TitiNote  \n",
       "515                  101  Délivrance de ceintures de flanelle  \n",
       "516                  102                             Dr Suard  \n",
       "517                  103                          NolletÉtude  \n",
       "518                  104                         Dr SoulsNote  \n",
       "\n",
       "[519 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualiser dans un même tableau l'ensemble des entités récupérées\n",
    "df_ents_default = pd.DataFrame(\n",
    "    {\"LOC\":LOCentities+list(range(9)), #Rééquilibrage pour pouvoir  \n",
    "     \"ORG\":ORGentities+list(range(426)),\n",
    "     \"PER\":PERentities+list(range(105)),\n",
    "     \"MISC\":MISCentities})\n",
    "df_ents_default.to_csv(\"./Notebook_data/Training_Doccano/df_ents_default_AHMC&AMN.tsv\",sep=\"\\t\")\n",
    "df_ents_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Commentaire du nlp par défaut de Spacy</b> :\n",
    "\n",
    "* `LOC` :\n",
    "    * Trop de `<GPE>` considérés comme `<LOC>` \n",
    "    * \"Variété\" pris comme `<LOC>`   \n",
    "\n",
    "* Beaucoup trop de `<MISC>` se référençant tant à des personnes qu'à des lieux,des termes sans aucun sens. \n",
    "\n",
    "* Pb annotation comme \"Paludisme\" en `<ORG>` \n",
    "\n",
    "* La NER ne respecte pas bien la tokenisation,notamment dans les cas où il y a un trait d'union.\n",
    "Ex : \"Ssé\" -\"Mao\" au lieu de \"Ssé-Mao\".\n",
    "\n",
    "\n",
    "Ces observations justifient bien la nécessité de ne pas s'en tenir au NLP de Spacy et à entraîner notre propre NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.fr import French\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PREPARATION DES FICHIERS POUR DOCCANO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Création du 4-fold set pour Doccano \n",
    "\n",
    "**Rappel:**\n",
    "Nombre d'articles dans chaque revue de 1898 à 1908: \n",
    "<br><u>AHMC</u> : 559 \n",
    "<br><u>AMN</u> :  638 \n",
    "<br><b><u>Total à traiter :  1197</u></b>\n",
    "\n",
    "En prenant un test_set size de 25%, on va donc chercher à avoir un 4-folds cross validation.Nous justifions ici le choix d'une cross-validation en raison de la \"sécurité\" qu'elle apporte par rapport à un \"simple\" training/test set. \n",
    "\n",
    "**Documentation utilisée** : <br>\n",
    "Cross-validation: \n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\n",
    "Tuto train_set_split: https://www.youtube.com/watch?v=ZHPwPHkrT4I\n",
    "\n",
    "\n",
    "ARTICLE À LIRE : https://towardsdatascience.com/complete-guide-to-pythons-cross-validation-with-examples-a9676b5cac12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies scikit-learn \n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Import des libraries pour visualiser les datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Création du 4-folds set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_set = df_spacyAHMC.article_titre.tolist() + df_spacyAMN.article_titre.tolist()\n",
    "df_data_set = pd.DataFrame({\"articles_AHMC_AMN\":data_set})\n",
    "\n",
    "#on choisit un modèle de 4-fold avec Shuffle car nos données sont pas ordre chronologique et par revue\n",
    "kf4 = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "#Visualisation d'une partie des indexs de la division\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Création de dictionnaires contenant chaque sous-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###ictionnaire avec les indexs de chaque article: \n",
    "n=1\n",
    "\n",
    "dico_sets_index= {}\n",
    "\n",
    "for train_index, test_index in kf4.split(data_set):\n",
    "    n_train=\"train_set\"+\"#\"+str(n)\n",
    "    n_test=\"test_set\"+\"#\"+str(n)\n",
    "    dico_sets_index[n_train]= train_index\n",
    "    dico_sets_index[n_test]= test_index\n",
    "    n+=1\n",
    "    #print(\"train:\",train_index[:15],\"\\nTest\",test_index[:15])\n",
    "    \n",
    "# print(dico_sets_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Dictionnaire avec les titres d'article \n",
    "\n",
    "dico_sets_articles= {}\n",
    "\n",
    "n=1\n",
    "for train_index, test_index in kf4.split(data_set):\n",
    "    n_train=\"train_set\"+str(n)\n",
    "    n_test=\"test_set\"+str(n)\n",
    "    dico_sets_articles[n_train]= np.take(data_set,train_index)\n",
    "    dico_sets_articles[n_test]= np.take(data_set,test_index)\n",
    "    n+=1\n",
    "len(dico_sets_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exportation des textes de train/test sets sous forme de liste :\n",
    "\n",
    "for key in dico_sets_articles:\n",
    "    output = \"./Notebook_data/Training_Doccano/train-test_sets_articles/\"+key\n",
    "    with open (output,\"w\") as f:\n",
    "        for article in dico_sets_articles[key]:\n",
    "            f.write(article)\n",
    "            f.write(\" \\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Visualisation du KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation du 4-fold:\n",
    "from Training_Doccano import visualize_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for train_index, test_index in kf4.split(df_data_set):\n",
    "#     X_train = df_data_set.iloc[train_index].loc[:, features]\n",
    "#     X_test = df_data_set.iloc[test_index].loc[:,features]\n",
    "#     y_train = df_data_set.iloc[train_index].loc[:,'target']\n",
    "#     y_test = df_data_set.loc[test_index].loc[:,'target']# visualize_groups(y, groups, 'no groups')\n",
    "\n",
    "# plt.scatter(x=y_train.index,y=df_data_set.iloc[train_index].loc[:,'target_name'],label =\"train\")\n",
    "# plt.scatter(x=y_test.index,y=df_data_set.iloc[test_index].loc[:,'target_name'], label = \"test\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Conversion des folds en JSONL pour Doccano\n",
    "\n",
    "#### Transformation de chaque df dans un format compatible avec l'annotation de Doccano \n",
    "<br>\n",
    "Avant d'utiliser Doccano pour corriger les annotations automatiques faites par Spacy, il est nécéssaire de pouvoir les importer dans ce premier. Autrement dit, les annotations contenues dans l'objet Doc de Spacy seront transformées dans un format lisible par Doccano, il s'agit ici du format JSONL, une forme de JSON. Le résultat sera enregistré dans un fichier qui sera dans le même dossier.\n",
    "\n",
    "<br></br>\n",
    "Comme j'ai recours à la Kfoldclassification, je dois extraire pour chaque partie du kfold un document JSONL exploitable sur Doccano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train_set1 created!\n",
      "File test_set1 created!\n",
      "File train_set2 created!\n",
      "File test_set2 created!\n",
      "File train_set3 created!\n",
      "File test_set3 created!\n",
      "File train_set4 created!\n",
      "File test_set4 created!\n"
     ]
    }
   ],
   "source": [
    "from Training_Doccano import dicoKfolds_to_doccano\n",
    "dicoKfolds_to_doccano(dico_sets_articles)\n",
    "\n",
    "###NB: comme la fonction write dans le module est en mode \"append\", \n",
    "### j'ai créé un autre directory \"kfolds-fixe\" qui est celui de référence \n",
    "### pour éviter d'utiliser des documents qui auraient été indentés trop de fois "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Correction des annotations sur Doccano \n",
    "\n",
    "1/Lancer l'Environnement virtuel (depuis ~)\n",
    "* `cd Python-virtual_environment`\n",
    "* `source ~/Python-virtual_environment/env-virtuel/bin/activate`\n",
    "=> s'assurer qu'on a bien 'env-virtuel') au début de la ligne de commande\n",
    "\n",
    "2/Lancer Doccano dans le terminal: \n",
    "* Ecrire \"`Doccano`\" dans le terminal\n",
    "\n",
    "aller sur : http://localhost:8000/. \n",
    "\n",
    "Sources utiles : \n",
    "\n",
    "* https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218 \n",
    "\n",
    "* https://docs.nlpcloud.io/#introduction\n",
    "\n",
    "* Evaluation:http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Labels\n",
    "\n",
    "Labels : \n",
    "\n",
    "> * `<LOC>` - Non-GPE locations, mountain ranges, bodies of water\n",
    "> * `<GPE>` - Geopolitical entity \n",
    "> * `<PATH>` - Nom d'affection pathologique (paludisme, béribéri, Peste..) et les substatifs associés (lépreux, pestiféré etc.) \n",
    "> * `<ETHNO>` - Reference à une pratique du terrain : \"Géographie médicale\", \"race\", \"coutume\" etc.\n",
    "> * `<CHEM>` - Les noms de substances chimiques et pharmaceutiques.\n",
    "\n",
    "Important : \n",
    "\n",
    "il y a de nombreux noms d'organisation qui contiennent des noms de `GPE/LOC` ou de `PATH` comme par exemple, \"Mission du Baoulé\" ou \"Mission du paludisme\". Etant donné mon axe de recherche, je fais le choix (certes criticable) de faire primer le sens de la `GPE/LOC` / `PATH` sur le nom de l' `ORG ` car c'est souvent le seul indice dans le titre de l'article sur l'affection ou le lieu mentionné. \n",
    "\n",
    "<br>\n",
    "Problème : certains termes qui désignent une pathologie \n",
    "\n",
    "Le fichier des labels se trouve dans le working directory. <br>Path : `./DOCCANO/doccano-input/project_labels.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ENTRAÎNEMENT DU NLP \n",
    "\n",
    "Chaque sous-partie correspond à une étape du cross validation et comprend les trois étapes suivantes:\n",
    "- Entrainement du nlp avec les annotations sur le training set\n",
    "- Validation \n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. KFOLD 1\n",
    "\n",
    "Documents du dataset associés : train_set1.jsonl et test_set1.jsonl\n",
    "\n",
    "### 3.1.1. entrainement du nlp avec les annotations sur le training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import du JSONL annoté sur Doccano\n",
    "json_doc_path = \"./DOCCANO/doccano-output/train_set1-annotatedd.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humanum/anaconda3/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"Épidémie de béribéri observée à Poulo-Condore en 1...\" with entities \"[(32, 46, 'GPE'), (12, 20, 'PATH')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"Fonctionnement de l'Institut Pasteur de Tananarive...\" with entities \"[(20, 51, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"Rapport sur les expériences comparatives de désinf...\" with entities \"[(125, 132, 'GPE'), (72, 121, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/humanum/anaconda3/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"Fonctionnement du service médical du « Peï-Ho », p...\" with entities \"[(39, 46, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "iteration: 2\n",
      "training is finished\n",
      "Saved model to Notebook_data/Training_Doccano/trained_nlp/nlp1\n"
     ]
    }
   ],
   "source": [
    "#Entraînement de ` fr_core_news_md ` avec les annotations stockées dans ` TRAIN_DATA `\n",
    "\n",
    "from Training_Doccano import train_data\n",
    "from Training_Doccano import train_nlp\n",
    "\n",
    "TRAIN_DATA=train_data(json_doc_path)\n",
    "\n",
    "train_nlp(\"nlp1\",TRAIN_DATA) \n",
    "# TRAIN_DATA\n",
    "\n",
    "#pb de Misaligned entities... signifie que j'ai du annoter de manière peu respectueuse des tokens préalables..\n",
    "#il n'y en a \"que\" 4 donc je laisse passer pour cette fois. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je veux aussi voir ce que cela donne sur des articles de la BSPE aussi de 1908 à 1914 (l'enjeu est ici de voir si ça prend bien en considération le vocabulaire plus techniques de la SPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
